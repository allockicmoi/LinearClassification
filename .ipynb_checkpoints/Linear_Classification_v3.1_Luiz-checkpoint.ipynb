{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "### Base code: Julien Courbebaisse\n",
    "### Revisions and modifications: Matheus Faria and Luiz Resende Silva "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code was implemented using two main libraries to handle data and perform the calculations: Pandas and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the function was implemented to be used to import the data sets, remove malformed values and extract basic statistics and pairwise correlation coerfficients between the columns (features and true labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File_DF(File_Name, separation = \",\", head = None, replace = [], drop=True):\n",
    "    try:\n",
    "        separation = separation.lower()\n",
    "        if(separation == \"space\" or separation == \"tab\"):\n",
    "            separation = \"\\t\"\n",
    "        Raw_Data_Set = pd.read_csv(File_Name, delimiter=separation, header=head, na_values=replace)\n",
    "        RawRowsColumns = Raw_Data_Set.shape\n",
    "        if(replace != None):\n",
    "            Missing = Raw_Data_Set.isnull().sum().sum()\n",
    "            print(\"Total number of missing/anomalous 'entries' in the data set: \",Missing)\n",
    "            if(drop == True):\n",
    "                Raw_Data_Set.dropna(axis=0, how='any', inplace=True)\n",
    "                CleanRowsColumns = Raw_Data_Set.shape\n",
    "                print(\"Number of examples with missing values deleted from data set: \",(RawRowsColumns[0]-CleanRowsColumns[0]))\n",
    "        Data_Set = Raw_Data_Set.to_numpy()\n",
    "        return Data_Set\n",
    "    except:\n",
    "        print(\"READ_FILE_ERROR\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other few functions were implemented to perform the binary classifiacation of the true labels from the data sets. The function were designed to be more general and allow modifications on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function replaces both by boundary (False) and fixed values (True) - each will be used in a different data set\n",
    "def replace_class(vector, index, flag=False):\n",
    "    if(flag == True):\n",
    "        if vector[index]==4:\n",
    "            vector[index]=1\n",
    "            return vector\n",
    "        elif(vector[index]==2):\n",
    "            vector[index]=0\n",
    "            return vector\n",
    "    elif(flag == False):\n",
    "        if (vector[index]<=5):\n",
    "            vector[index]=0\n",
    "            return vector\n",
    "        elif(vector[index]>5):\n",
    "            vector[index]=1\n",
    "            return vector\n",
    "\n",
    "#Function either adds (False) or substitutes (True) column by the intercept bias weights\n",
    "def substi_add(dataset, index, flag=True): #Adding column of 1 to determine bias term\n",
    "    if(flag==True):\n",
    "        for element in dataset:\n",
    "            element[index] = 1\n",
    "        return dataset\n",
    "    else:\n",
    "        temp = dataset\n",
    "        temp = np.insert(temp, 0, 1, axis=1)\n",
    "        return temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(Dataset, Index1, Index2=None, Flag=True):\n",
    "    if(Flag==True):\n",
    "        for element in Dataset:\n",
    "            element = replace_class(element,index=Index1, flag=Flag)\n",
    "        Dataset = substi_add(Dataset, index=Index2, flag=Flag)\n",
    "        return Dataset\n",
    "    else:\n",
    "        for element in Dataset:\n",
    "            element = replace_class(element,index=Index1, flag=Flag)\n",
    "        Dataset = substi_add(Dataset, index=Index2, flag=Flag)\n",
    "        return Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Another function was implemented to retrieve information about the data sets, such as general statistics and Spearman Rank Correlation between columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Stats(Data, QQ_DD = True, show = True):\n",
    "    try:\n",
    "        Data_Set = pd.DataFrame(Data, index = (list(range(0,Data.shape[0]))), columns = (list(range(0,Data.shape[1]))))\n",
    "        if(QQ_DD == True):           \n",
    "            quantiles = [0.00, 0.25, 0.50, 0.75] #Calculating quartiles\n",
    "        else:\n",
    "            quantiles = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00] #Calculating quartiles\n",
    "\n",
    "        Describ = Data_Set.describe(percentiles = quantiles) #Data set general stats description\n",
    "        Correlation = Data_Set.corr('spearman') #Computing pairwise feature correlation through Spearman rank correlation\n",
    "        name = (\"GeneralStats.xlsx\")\n",
    "        with pd.ExcelWriter(name) as writer: #Outputting Excel file with statistics\n",
    "            Describ.to_excel(writer, sheet_name='Data_Description')\n",
    "            Correlation.to_excel(writer, sheet_name='Column_Correlation')\n",
    "        if(show == True):\n",
    "            print(Data_Set)\n",
    "            print(Describ) #Printing statistics to screen\n",
    "            print(Correlation) #Printing statistics to screen\n",
    "    except:\n",
    "        print(\"STATS_FUNCTION_ERROR\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Linear Classification model was implemented as a Python class. The class was called logistiReg and all the functions needed to perform the Logistic Regression were implemented inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticReg(object):\n",
    "    \"\"\"Class to build logistic regression model\"\"\"\n",
    "        \n",
    "    #Constructor called passing data set\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    #Sigmoid function\n",
    "    def sigmoid(self, a):\n",
    "        sigma = (1/(1+(np.exp(-a))))\n",
    "        return sigma\n",
    "        \n",
    "    #Update rule function\n",
    "    def update(self, weights, dataset, Lrate):\n",
    "        new_weights=weights #Initializing weights vector\n",
    "        suma=0\n",
    "        for element in dataset:\n",
    "            suma += (((element[(self.Features_Selec)])*((element[(self.Outcome_Col)])-(self.sigmoid(new_weights.dot(element[(self.Features_Selec)]))))))\n",
    "        new_weights = np.add(new_weights, (Lrate*suma)) #Updating weights\n",
    "        return new_weights\n",
    "\n",
    "    #Function to calculate the Cross-Entropy Loss\n",
    "    def Cross_Entropy(self , data, weight):\n",
    "        CE = 0\n",
    "        for element in data:\n",
    "            Term1 = np.dot(element[(self.Outcome_Col)], np.log(self.sigmoid(weight.dot(element[(self.Features_Selec)]))))\n",
    "            Term2 = np.dot((1 - element[(self.Outcome_Col)]),np.log(1 - self.sigmoid(weight.dot(element[(self.Features_Selec)]))))\n",
    "            CE += - np.add(Term1,Term2)\n",
    "        return CE\n",
    "    \n",
    "    \n",
    "    #Fitting model function    \n",
    "    def fit_model(self, Training_Set, lstFeatures, outCol, rate=0.00001, maxIter=1000, reduct=1, ran=False):\n",
    "        self.Features_Selec = lstFeatures #List of features selected to fit the model\n",
    "        self.Outcome_Col = outCol #Index of outcome column\n",
    "        \n",
    "        if(ran == True):\n",
    "            w = np.random.randint(0,10,len(self.Features_Selec)) #Starting vector of weights with random numbers between 0-10\n",
    "        else:\n",
    "            w = np.zeros(len(self.Features_Selec)) #Starting vector of weights with zeros\n",
    "        CE_D = []\n",
    "        i=0\n",
    "        while i < maxIter:\n",
    "            w = self.update(weights=w, dataset=Training_Set, Lrate=rate)\n",
    "            cost = self.Cross_Entropy(data=Training_Set, weight=w)\n",
    "            CE_D.append(cost)\n",
    "            rate = rate/reduct\n",
    "            i+=1\n",
    "#        print(CE_D)\n",
    "        return w\n",
    "    \n",
    "    #Function to use the weights calculated to perform predictions\n",
    "    def predict(self, data, weights):\n",
    "        truelabel = []\n",
    "        predicted = []\n",
    "        compare = []\n",
    "        for example in data:\n",
    "            realpred = np.dot(example[self.Features_Selec],weights)\n",
    "            if(realpred <= 0.5):\n",
    "                predicted.append(int(0))\n",
    "            else:\n",
    "                predicted.append(int(1))\n",
    "            truelabel.append(int(example[self.Outcome_Col]))\n",
    "        \n",
    "        compare.append(truelabel)\n",
    "        compare.append(predicted)\n",
    "        return compare    \n",
    "\n",
    "    def evaluate_acc(self, Preds):\n",
    "        score = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        Predictions = np.transpose(Preds)\n",
    "        for pred in Predictions:\n",
    "            if pred[1]==pred[0]:\n",
    "                score+=1\n",
    "                if(pred[1]==1):\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if(pred[1]==0):\n",
    "                    FN += 1\n",
    "        \n",
    "        print(\"Correct Predictions: \",score)\n",
    "        print(\"Total Predictions: \",len(Predictions))\n",
    "        print(\"Accuracy Percentage: \",round(((score/len(Predictions))*100),4),\"%\")\n",
    "        accur = ((score/len(Predictions))*100)\n",
    "        if(TP>0 or FN>0):\n",
    "            print(\"Sensitivity Percentage: \",round(((TP/(TP+FN))*100),4),\"%\")\n",
    "            sense = ((TP/(TP+FN))*100)\n",
    "        else:\n",
    "            print(\"Sensitivity Percentage: 0.0000%\")\n",
    "            sense = 0\n",
    "        return accur, sense\n",
    "    \n",
    "    def kfold(self,k, data, LstFeatures, OutCol, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False):\n",
    "        i=0\n",
    "        score1=0\n",
    "        score2=0\n",
    "        batch_size=np.floor(data[:,1].size/k)\n",
    "        while i < k: \n",
    "            if(i<k-1):\n",
    "                training_data=np.delete(data, np.s_[(int)(i*batch_size):(int)((i+1)*batch_size)], 0)\n",
    "                test_data= data[(int)(i*(batch_size)):(int)((i+1)*(batch_size))]\n",
    "                weights = self.fit_model(training_data, LstFeatures, OutCol, rate=Rate, maxIter=MaxIter, reduct=Reduct, ran=Ran)\n",
    "                preds = self.predict(test_data, weights)\n",
    "                print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "                temp1, temp2 = self.evaluate_acc(preds)\n",
    "                score1+=temp1\n",
    "                score2+=temp2\n",
    "                i+=1\n",
    "            else:\n",
    "                training_data=np.delete(data, np.s_[(int)(i*batch_size):], 0)\n",
    "                test_data= data[(int)(i*(batch_size)):]\n",
    "                weights = self.fit_model(training_data, LstFeatures, OutCol, rate=Rate, maxIter=MaxIter, reduct=Reduct, ran=Ran)\n",
    "                preds = self.predict(test_data, weights)\n",
    "                print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "                temp1, temp2 = self.evaluate_acc(preds)\n",
    "                score1+=temp1\n",
    "                score2+=temp2\n",
    "                i+=1\n",
    "        print(\"\\n##### RESULTS FOR THE kFOLD CROSS VALIDATION WERE #####\")\n",
    "        print(\"Average Accuracy:\",round((score1/k),3),\"%\")\n",
    "        print(\"Average Sensitivity:\",round((score2/k),3),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above class and functions, we can perform the Logistic Regression in each of the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the breast cancer data set, only 16 instances were found with incorrect values (\"?\"). These examples were removed from the data set when importing by flagging these \"?\" characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing/anomalous 'entries' in the data set:  0\n",
      "Number of examples with missing values deleted from data set:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1000025,       5,       1, ...,       1,       1,       2],\n",
       "       [1002945,       5,       4, ...,       2,       1,       2],\n",
       "       [1015425,       3,       1, ...,       1,       1,       2],\n",
       "       ...,\n",
       "       [ 888820,       5,      10, ...,      10,       2,       4],\n",
       "       [ 897471,       4,       8, ...,       6,       1,       4],\n",
       "       [ 897471,       4,       8, ...,       4,       1,       4]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data and remove missing values\n",
    "filename = 'breast-cancer-wisconsin.data'\n",
    "databc = Read_File_DF(filename, separation=',', head=None, replace=[\"?\"], drop=True)\n",
    "databc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the breast cancer data, the first column referent to the sample IDs was replaced by the intercept values since these information was not needed. Therefore, for this data set, the first weight in the vector $ w_{k}$ containing $k=m+1$ weights will be the intercept. The resulting data matrix will have6 683 rows and 11 columns: 9 features plus 1 intercept and the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  1, ...,  1,  1,  0],\n",
       "       [ 1,  5,  4, ...,  2,  1,  0],\n",
       "       [ 1,  3,  1, ...,  1,  1,  0],\n",
       "       ...,\n",
       "       [ 1,  5, 10, ..., 10,  2,  1],\n",
       "       [ 1,  4,  8, ...,  6,  1,  1],\n",
       "       [ 1,  4,  8, ...,  4,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databc = preproc(databc, Index1=10, Index2=0, Flag=True)\n",
    "databc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATS_FUNCTION_ERROR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Data_Stats(databc, QQ_DD = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Calling constructor for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelBC = logisticReg(databc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of features to be selected (_always between 0-9 with the 0 included_ - $w_{0}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [0,1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of examples to be used in training and validating the model ( _for uses outside kFold method_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = databc[:400,:]\n",
    "testing = databc[500:750,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model outside kFold method ( _single training and validation_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in: 13.6292988 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.5788927 , -0.06935013,  0.2952298 ,  0.19996855,  0.12166342,\n",
       "       -0.2609533 ,  0.33088139, -0.42333995,  0.17030403, -0.07630531,\n",
       "        0.30812813])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer() #Starting clock to count time for training\n",
    "\n",
    "weights = modelBC.fit_model(Training_Set=training, lstFeatures=feats, outCol=10, rate=0.00001, maxIter=1000, reduct=1, ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Model trained in:\",stop-start,\"s\")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = modelBC.predict(testing, weights)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Predictions:  174\n",
      "Total Predictions:  183\n",
      "Accuracy Percentage:  95.082 %\n",
      "Sensitivity Percentage:  83.3333 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95.08196721311475, 83.33333333333334)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBC.evaluate_acc(Preds=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running kFolda Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation # 1  Results:\n",
      "Correct Predictions:  109\n",
      "Total Predictions:  136\n",
      "Accuracy Percentage:  80.1471 %\n",
      "Sensitivity Percentage:  62.2951 %\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer() #Starting clock\n",
    "\n",
    "modelBC.kfold(k=5,data=databc, LstFeatures=feats, OutCol=10, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"kFold validation for Breast Cancer data ser performed in:\",stop-start,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Wine Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the red wine quality data set, no instances with incorrect of missing values were found. Therefore, no example was removed from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and remove missing values\n",
    "filename = 'winequality_red.csv'\n",
    "datawine = Read_File_DF(filename, separation=';', head=0, replace=[\"?\"], drop=False)\n",
    "datawine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, the intercept value weights $ w_{0}$ were added as a new column to the beginning of the matrix. The resulting data matrix will have6 1599 rows and 12 columns: 10 features plus 1 intercept and the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datawine = preproc(datawine, Index1=11, Index2=None, Flag=False)\n",
    "datawine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data_Stats(datawine, QQ_DD = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Calling constructor for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelWine = logisticReg(datawine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of features to be selected (_always between 0-10 with the 0 included_ - $w_{0}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [0,1,2,3,4,5,6,7,8,9,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of examples to be used in training and validating the model ( _for uses outside kFold method_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = datawine[250:750,:]\n",
    "testing = datawine[1000:1350,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model outside kFold method ( _single training and validation_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer() #Starting clock to count time for training\n",
    "\n",
    "weights = modelWine.fit_model(Training_Set=training, lstFeatures=feats, outCol=12, rate=0.00001, maxIter=100, reduct=1, ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Model trained in:\",stop-start,\"s\")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelWine.predict(testing, weights)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelWine.evaluate_acc(Preds=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running kFolda Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\luiza\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation # 1  Results:\n",
      "Correct Predictions:  197\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  61.7555 %\n",
      "Sensitivity Percentage:  4.8387 %\n",
      "\n",
      "Segmentation # 2  Results:\n",
      "Correct Predictions:  207\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  64.8903 %\n",
      "Sensitivity Percentage:  93.8462 %\n",
      "\n",
      "Segmentation # 3  Results:\n",
      "Correct Predictions:  167\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  52.3511 %\n",
      "Sensitivity Percentage:  98.0132 %\n",
      "\n",
      "Segmentation # 4  Results:\n",
      "Correct Predictions:  225\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  70.5329 %\n",
      "Sensitivity Percentage:  93.2432 %\n",
      "\n",
      "Segmentation # 5  Results:\n",
      "Correct Predictions:  163\n",
      "Total Predictions:  323\n",
      "Accuracy Percentage:  50.4644 %\n",
      "Sensitivity Percentage:  95.092 %\n",
      "\n",
      "##### RESULTS FOR THE kFOLD CROSS VALIDATION WERE #####\n",
      "Average Accuracy: 59.999 %\n",
      "Average Sensitivity: 77.007 %\n",
      "kFold validation for Wine Data set performed in: 475.23168780000015 s\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer() #Starting clock\n",
    "\n",
    "modelWine.kfold(k=5,data=datawine, LstFeatures=feats, OutCol=12, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"kFold validation for Wine Data set performed in:\",stop-start,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
