{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "### Base code: Julien Courbebaisse\n",
    "### Revisions and modifications: Matheus Faria and Luiz Resende Silva "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code was implemented using two main libraries to handle data and perform the calculations: Pandas and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the function was implemented to be used to import the data sets, remove malformed values and extract basic statistics and pairwise correlation coerfficients between the columns (features and true labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File_DF(File_Name, separation = \",\", head = None, replace = [], drop=True):\n",
    "    try:\n",
    "        separation = separation.lower()\n",
    "        if(separation == \"space\" or separation == \"tab\"):\n",
    "            separation = \"\\t\"\n",
    "        Raw_Data_Set = pd.read_csv(File_Name, delimiter=separation, header=head, na_values=replace)\n",
    "        RawRowsColumns = Raw_Data_Set.shape\n",
    "        if(replace != None):\n",
    "            Missing = Raw_Data_Set.isnull().sum().sum()\n",
    "            print(\"Total number of missing/anomalous 'entries' in the data set: \",Missing)\n",
    "            if(drop == True):\n",
    "                Raw_Data_Set.dropna(axis=0, how='any', inplace=True)\n",
    "                CleanRowsColumns = Raw_Data_Set.shape\n",
    "                print(\"Number of examples with missing values deleted from data set: \",(RawRowsColumns[0]-CleanRowsColumns[0]))\n",
    "        Data_Set = Raw_Data_Set.to_numpy()\n",
    "        return Data_Set\n",
    "    except:\n",
    "        print(\"READ_FILE_ERROR\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other few functions were implemented to perform the binary classifiacation of the true labels from the data sets. The function were designed to be more general and allow modifications on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function replaces both by boundary (False) and fixed values (True) - each will be used in a different data set\n",
    "def replace_class(vector, index, flag=False):\n",
    "    if(flag == True):\n",
    "        if vector[index]==4:\n",
    "            vector[index]=1\n",
    "            return vector\n",
    "        elif(vector[index]==2):\n",
    "            vector[index]=0\n",
    "            return vector\n",
    "    elif(flag == False):\n",
    "        if (vector[index]<=5):\n",
    "            vector[index]=0\n",
    "            return vector\n",
    "        elif(vector[index]>5):\n",
    "            vector[index]=1\n",
    "            return vector\n",
    "\n",
    "#Function either adds (False) or substitutes (True) column by the intercept bias weights\n",
    "def substi_add(dataset, index, flag=True): #Adding column of 1 to determine bias term\n",
    "    if(flag==True):\n",
    "        for element in dataset:\n",
    "            element[index] = 1\n",
    "        return dataset\n",
    "    else:\n",
    "        temp = dataset\n",
    "        temp = np.insert(temp, 0, 1, axis=1)\n",
    "        return temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(Dataset, Index1, Index2=None, Flag=True):\n",
    "    if(Flag==True):\n",
    "        for element in Dataset:\n",
    "            element = replace_class(element,index=Index1, flag=Flag)\n",
    "        Dataset = substi_add(Dataset, index=Index2, flag=Flag)\n",
    "        return Dataset\n",
    "    else:\n",
    "        for element in Dataset:\n",
    "            element = replace_class(element,index=Index1, flag=Flag)\n",
    "        Dataset = substi_add(Dataset, index=Index2, flag=Flag)\n",
    "        return Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Another function was implemented to retrieve information about the data sets, such as general statistics and Spearman Rank Correlation between columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Stats(Data, QQ_DD = True, show = True):\n",
    "    try:\n",
    "        Data_Set = pd.DataFrame(Data, index = (list(range(0,Data.shape[0]))), columns = (list(range(0,Data.shape[1]))))\n",
    "        if(QQ_DD == True):           \n",
    "            quantiles = [0.00, 0.25, 0.50, 0.75] #Calculating quartiles\n",
    "        else:\n",
    "            quantiles = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00] #Calculating quartiles\n",
    "\n",
    "        Describ = Data_Set.describe(percentiles = quantiles) #Data set general stats description\n",
    "        Correlation = Data_Set.corr('spearman') #Computing pairwise feature correlation through Spearman rank correlation\n",
    "        name = (\"GeneralStats.xlsx\")\n",
    "        with pd.ExcelWriter(name) as writer: #Outputting Excel file with statistics\n",
    "            Describ.to_excel(writer, sheet_name='Data_Description')\n",
    "            Correlation.to_excel(writer, sheet_name='Column_Correlation')\n",
    "        if(show == True):\n",
    "            print(Data_Set)\n",
    "            print(Describ) #Printing statistics to screen\n",
    "            print(Correlation) #Printing statistics to screen\n",
    "    except:\n",
    "        print(\"STATS_FUNCTION_ERROR\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Linear Classification model was implemented as a Python class. The class was called logistiReg and all the functions needed to perform the Logistic Regression were implemented inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticReg(object):\n",
    "    \"\"\"Class to build logistic regression model\"\"\"\n",
    "        \n",
    "    #Constructor called passing data set\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    #Sigmoid function\n",
    "    def sigmoid(self, a):\n",
    "        sigma = (1/(1+(np.exp(-a))))\n",
    "        return sigma\n",
    "        \n",
    "    #Update rule function\n",
    "    def update(self, weights, dataset, Lrate):\n",
    "        new_weights=weights #Initializing weights vector\n",
    "        suma=0\n",
    "        for element in dataset:\n",
    "            suma += (((element[(self.Features_Selec)])*((element[(self.Outcome_Col)])-(self.sigmoid(new_weights.dot(element[(self.Features_Selec)]))))))\n",
    "        new_weights = np.add(new_weights, (Lrate*suma)) #Updating weights\n",
    "        return new_weights\n",
    "\n",
    "    #Function to calculate the Cross-Entropy Loss\n",
    "    def Cross_Entropy(self , data, weight):\n",
    "        CE = 0\n",
    "        for element in data:\n",
    "            Term1 = np.dot(element[(self.Outcome_Col)], np.log(self.sigmoid(weight.dot(element[(self.Features_Selec)]))))\n",
    "            Term2 = np.dot((1 - element[(self.Outcome_Col)]),np.log(1 - self.sigmoid(weight.dot(element[(self.Features_Selec)]))))\n",
    "            CE += - np.add(Term1,Term2)\n",
    "        return CE\n",
    "    \n",
    "    \n",
    "    #Fitting model function    \n",
    "    def fit_model(self, Training_Set, lstFeatures, outCol, rate=0.00001, maxIter=1000, reduct=1, ran=False):\n",
    "        self.Features_Selec = lstFeatures #List of features selected to fit the model\n",
    "        self.Outcome_Col = outCol #Index of outcome column\n",
    "        \n",
    "        if(ran == True):\n",
    "            w = np.random.randint(0,10,len(self.Features_Selec)) #Starting vector of weights with random numbers between 0-10\n",
    "        else:\n",
    "            w = np.zeros(len(self.Features_Selec)) #Starting vector of weights with zeros\n",
    "        CE_D = []\n",
    "        i=0\n",
    "        while i < maxIter:\n",
    "            w = self.update(weights=w, dataset=Training_Set, Lrate=rate)\n",
    "            cost = self.Cross_Entropy(data=Training_Set, weight=w)\n",
    "            CE_D.append(cost)\n",
    "            rate = rate/reduct\n",
    "            i+=1\n",
    "#        print(CE_D)\n",
    "        return w\n",
    "    \n",
    "    #Function to use the weights calculated to perform predictions\n",
    "    def predict(self, data, weights):\n",
    "        truelabel = []\n",
    "        predicted = []\n",
    "        compare = []\n",
    "        for example in data:\n",
    "            realpred = np.dot(example[self.Features_Selec],weights)\n",
    "            if(realpred <= 0.5):\n",
    "                predicted.append(int(0))\n",
    "            else:\n",
    "                predicted.append(int(1))\n",
    "            truelabel.append(int(example[self.Outcome_Col]))\n",
    "        \n",
    "        compare.append(truelabel)\n",
    "        compare.append(predicted)\n",
    "        return compare    \n",
    "\n",
    "    def evaluate_acc(self, Preds):\n",
    "        score = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        Predictions = np.transpose(Preds)\n",
    "        for pred in Predictions:\n",
    "            if pred[1]==pred[0]:\n",
    "                score+=1\n",
    "                if(pred[1]==1):\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if(pred[1]==0):\n",
    "                    FN += 1\n",
    "        \n",
    "        print(\"Correct Predictions: \",score)\n",
    "        print(\"Total Predictions: \",len(Predictions))\n",
    "        print(\"Accuracy Percentage: \",round(((score/len(Predictions))*100),4),\"%\")\n",
    "        accur = ((score/len(Predictions))*100)\n",
    "        if(TP>0 or FN>0):\n",
    "            print(\"Sensitivity Percentage: \",round(((TP/(TP+FN))*100),4),\"%\")\n",
    "            sense = ((TP/(TP+FN))*100)\n",
    "        else:\n",
    "            print(\"Sensitivity Percentage: 0.0000%\")\n",
    "            sense = 0\n",
    "        return accur, sense\n",
    "    \n",
    "    def kfold(self,k, data, LstFeatures, OutCol, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False):\n",
    "        i=0\n",
    "        score1=0\n",
    "        score2=0\n",
    "        batch_size=np.floor(data[:,1].size/k)\n",
    "        while i < k: \n",
    "            if(i<k-1):\n",
    "                training_data=np.delete(data, np.s_[(int)(i*batch_size):(int)((i+1)*batch_size)], 0)\n",
    "                test_data= data[(int)(i*(batch_size)):(int)((i+1)*(batch_size))]\n",
    "                weights = self.fit_model(training_data, LstFeatures, OutCol, rate=Rate, maxIter=MaxIter, reduct=Reduct, ran=Ran)\n",
    "                preds = self.predict(test_data, weights)\n",
    "                print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "                temp1, temp2 = self.evaluate_acc(preds)\n",
    "                score1+=temp1\n",
    "                score2+=temp2\n",
    "                i+=1\n",
    "            else:\n",
    "                training_data=np.delete(data, np.s_[(int)(i*batch_size):], 0)\n",
    "                test_data= data[(int)(i*(batch_size)):]\n",
    "                weights = self.fit_model(training_data, LstFeatures, OutCol, rate=Rate, maxIter=MaxIter, reduct=Reduct, ran=Ran)\n",
    "                preds = self.predict(test_data, weights)\n",
    "                print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "                temp1, temp2 = self.evaluate_acc(preds)\n",
    "                score1+=temp1\n",
    "                score2+=temp2\n",
    "                i+=1\n",
    "        print(\"\\n##### RESULTS FOR THE kFOLD CROSS VALIDATION WERE #####\")\n",
    "        print(\"Average Accuracy:\",round((score1/k),3),\"%\")\n",
    "        print(\"Average Sensitivity:\",round((score2/k),3),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above class and functions, we can perform the Logistic Regression in each of the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the breast cancer data set, only 16 instances were found with incorrect values (\"?\"). These examples were removed from the data set when importing by flagging these \"?\" characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing/anomalous 'entries' in the data set:  16\n",
      "Number of examples with missing values deleted from data set:  16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.000025e+06, 5.000000e+00, 1.000000e+00, ..., 1.000000e+00,\n",
       "        1.000000e+00, 2.000000e+00],\n",
       "       [1.002945e+06, 5.000000e+00, 4.000000e+00, ..., 2.000000e+00,\n",
       "        1.000000e+00, 2.000000e+00],\n",
       "       [1.015425e+06, 3.000000e+00, 1.000000e+00, ..., 1.000000e+00,\n",
       "        1.000000e+00, 2.000000e+00],\n",
       "       ...,\n",
       "       [8.888200e+05, 5.000000e+00, 1.000000e+01, ..., 1.000000e+01,\n",
       "        2.000000e+00, 4.000000e+00],\n",
       "       [8.974710e+05, 4.000000e+00, 8.000000e+00, ..., 6.000000e+00,\n",
       "        1.000000e+00, 4.000000e+00],\n",
       "       [8.974710e+05, 4.000000e+00, 8.000000e+00, ..., 4.000000e+00,\n",
       "        1.000000e+00, 4.000000e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data and remove missing values\n",
    "filename = 'breast-cancer-wisconsin.data'\n",
    "databc = Read_File_DF(filename, separation=',', head=None, replace=[\"?\"], drop=True)\n",
    "databc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the breast cancer data, the first column referent to the sample IDs was replaced by the intercept values since these information was not needed. Therefore, for this data set, the first weight in the vector $ w_{k}$ containing $k=m+1$ weights will be the intercept. The resulting data matrix will have6 683 rows and 11 columns: 9 features plus 1 intercept and the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  5.,  1., ...,  1.,  1.,  0.],\n",
       "       [ 1.,  5.,  4., ...,  2.,  1.,  0.],\n",
       "       [ 1.,  3.,  1., ...,  1.,  1.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  5., 10., ..., 10.,  2.,  1.],\n",
       "       [ 1.,  4.,  8., ...,  6.,  1.,  1.],\n",
       "       [ 1.,  4.,  8., ...,  4.,  1.,  1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databc = preproc(databc, Index1=10, Index2=0, Flag=True)\n",
    "databc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4    5     6     7     8    9    10\n",
      "0    1.0   5.0   1.0   1.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "1    1.0   5.0   4.0   4.0   5.0  7.0  10.0   3.0   2.0  1.0  0.0\n",
      "2    1.0   3.0   1.0   1.0   1.0  2.0   2.0   3.0   1.0  1.0  0.0\n",
      "3    1.0   6.0   8.0   8.0   1.0  3.0   4.0   3.0   7.0  1.0  0.0\n",
      "4    1.0   4.0   1.0   1.0   3.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "5    1.0   8.0  10.0  10.0   8.0  7.0  10.0   9.0   7.0  1.0  1.0\n",
      "6    1.0   1.0   1.0   1.0   1.0  2.0  10.0   3.0   1.0  1.0  0.0\n",
      "7    1.0   2.0   1.0   2.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "8    1.0   2.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  5.0  0.0\n",
      "9    1.0   4.0   2.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "10   1.0   1.0   1.0   1.0   1.0  1.0   1.0   3.0   1.0  1.0  0.0\n",
      "11   1.0   2.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "12   1.0   5.0   3.0   3.0   3.0  2.0   3.0   4.0   4.0  1.0  1.0\n",
      "13   1.0   1.0   1.0   1.0   1.0  2.0   3.0   3.0   1.0  1.0  0.0\n",
      "14   1.0   8.0   7.0   5.0  10.0  7.0   9.0   5.0   5.0  4.0  1.0\n",
      "15   1.0   7.0   4.0   6.0   4.0  6.0   1.0   4.0   3.0  1.0  1.0\n",
      "16   1.0   4.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "17   1.0   4.0   1.0   1.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "18   1.0  10.0   7.0   7.0   6.0  4.0  10.0   4.0   1.0  2.0  1.0\n",
      "19   1.0   6.0   1.0   1.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "20   1.0   7.0   3.0   2.0  10.0  5.0  10.0   5.0   4.0  4.0  1.0\n",
      "21   1.0  10.0   5.0   5.0   3.0  6.0   7.0   7.0  10.0  1.0  1.0\n",
      "22   1.0   3.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "23   1.0   1.0   1.0   1.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "24   1.0   5.0   2.0   3.0   4.0  2.0   7.0   3.0   6.0  1.0  1.0\n",
      "25   1.0   3.0   2.0   1.0   1.0  1.0   1.0   2.0   1.0  1.0  0.0\n",
      "26   1.0   5.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "27   1.0   2.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "28   1.0   1.0   1.0   3.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "29   1.0   3.0   1.0   1.0   1.0  1.0   1.0   2.0   1.0  1.0  0.0\n",
      "..   ...   ...   ...   ...   ...  ...   ...   ...   ...  ...  ...\n",
      "653  1.0   5.0  10.0  10.0   8.0  5.0   5.0   7.0  10.0  1.0  1.0\n",
      "654  1.0   3.0  10.0   7.0   8.0  5.0   8.0   7.0   4.0  1.0  1.0\n",
      "655  1.0   3.0   2.0   1.0   2.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "656  1.0   2.0   1.0   1.0   1.0  2.0   1.0   3.0   1.0  1.0  0.0\n",
      "657  1.0   5.0   3.0   2.0   1.0  3.0   1.0   1.0   1.0  1.0  0.0\n",
      "658  1.0   1.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "659  1.0   4.0   1.0   4.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "660  1.0   1.0   1.0   2.0   1.0  2.0   1.0   2.0   1.0  1.0  0.0\n",
      "661  1.0   5.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "662  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "663  1.0   2.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "664  1.0  10.0  10.0  10.0  10.0  5.0  10.0  10.0  10.0  7.0  1.0\n",
      "665  1.0   5.0  10.0  10.0  10.0  4.0  10.0   5.0   6.0  3.0  1.0\n",
      "666  1.0   5.0   1.0   1.0   1.0  2.0   1.0   3.0   2.0  1.0  0.0\n",
      "667  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "668  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "669  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "670  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "671  1.0   3.0   1.0   1.0   1.0  2.0   1.0   2.0   3.0  1.0  0.0\n",
      "672  1.0   4.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "673  1.0   1.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  8.0  0.0\n",
      "674  1.0   1.0   1.0   1.0   3.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "675  1.0   5.0  10.0  10.0   5.0  4.0   5.0   4.0   4.0  1.0  1.0\n",
      "676  1.0   3.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "677  1.0   3.0   1.0   1.0   1.0  2.0   1.0   2.0   1.0  2.0  0.0\n",
      "678  1.0   3.0   1.0   1.0   1.0  3.0   2.0   1.0   1.0  1.0  0.0\n",
      "679  1.0   2.0   1.0   1.0   1.0  2.0   1.0   1.0   1.0  1.0  0.0\n",
      "680  1.0   5.0  10.0  10.0   3.0  7.0   3.0   8.0  10.0  2.0  1.0\n",
      "681  1.0   4.0   8.0   6.0   4.0  3.0   4.0  10.0   6.0  1.0  1.0\n",
      "682  1.0   4.0   8.0   8.0   5.0  4.0   5.0  10.0   4.0  1.0  1.0\n",
      "\n",
      "[683 rows x 11 columns]\n",
      "          0           1           2           3           4           5   \\\n",
      "count  683.0  683.000000  683.000000  683.000000  683.000000  683.000000   \n",
      "mean     1.0    4.442167    3.150805    3.215227    2.830161    3.234261   \n",
      "std      0.0    2.820761    3.065145    2.988581    2.864562    2.223085   \n",
      "min      1.0    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "0%       1.0    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "25%      1.0    2.000000    1.000000    1.000000    1.000000    2.000000   \n",
      "50%      1.0    4.000000    1.000000    1.000000    1.000000    2.000000   \n",
      "75%      1.0    6.000000    5.000000    5.000000    4.000000    4.000000   \n",
      "max      1.0   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
      "\n",
      "               6           7           8           9           10  \n",
      "count  683.000000  683.000000  683.000000  683.000000  683.000000  \n",
      "mean     3.544656    3.445095    2.869693    1.603221    0.349927  \n",
      "std      3.643857    2.449697    3.052666    1.732674    0.477296  \n",
      "min      1.000000    1.000000    1.000000    1.000000    0.000000  \n",
      "0%       1.000000    1.000000    1.000000    1.000000    0.000000  \n",
      "25%      1.000000    2.000000    1.000000    1.000000    0.000000  \n",
      "50%      1.000000    3.000000    1.000000    1.000000    0.000000  \n",
      "75%      6.000000    5.000000    4.000000    1.000000    1.000000  \n",
      "max     10.000000   10.000000   10.000000   10.000000    1.000000  \n",
      "    0         1         2         3         4         5         6         7   \\\n",
      "0  NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "1  NaN  1.000000  0.663684  0.666652  0.543815  0.586755  0.590770  0.533848   \n",
      "2  NaN  0.663684  1.000000  0.894978  0.745475  0.792814  0.769543  0.720595   \n",
      "3  NaN  0.666652  0.894978  1.000000  0.718722  0.765073  0.752950  0.694888   \n",
      "4  NaN  0.543815  0.745475  0.718722  1.000000  0.665190  0.696792  0.628738   \n",
      "5  NaN  0.586755  0.792814  0.765073  0.665190  1.000000  0.694579  0.645090   \n",
      "6  NaN  0.590770  0.769543  0.752950  0.696792  0.694579  1.000000  0.678963   \n",
      "7  NaN  0.533848  0.720595  0.694888  0.628738  0.645090  0.678963  1.000000   \n",
      "8  NaN  0.566398  0.752510  0.724410  0.636409  0.710628  0.659762  0.662096   \n",
      "9  NaN  0.421208  0.512688  0.478400  0.447254  0.483210  0.474316  0.390600   \n",
      "10 NaN  0.683080  0.860299  0.843245  0.737737  0.775066  0.835444  0.744571   \n",
      "\n",
      "          8         9         10  \n",
      "0        NaN       NaN       NaN  \n",
      "1   0.566398  0.421208  0.683080  \n",
      "2   0.752510  0.512688  0.860299  \n",
      "3   0.724410  0.478400  0.843245  \n",
      "4   0.636409  0.447254  0.737737  \n",
      "5   0.710628  0.483210  0.775066  \n",
      "6   0.659762  0.474316  0.835444  \n",
      "7   0.662096  0.390600  0.744571  \n",
      "8   1.000000  0.510210  0.748601  \n",
      "9   0.510210  1.000000  0.527379  \n",
      "10  0.748601  0.527379  1.000000  \n"
     ]
    }
   ],
   "source": [
    "Data_Stats(databc, QQ_DD = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Calling constructor for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelBC = logisticReg(databc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of features to be selected (_always between 0-9 with the 0 included_ - $w_{0}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [0,1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of examples to be used in training and validating the model ( _for uses outside kFold method_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = databc[:400,:]\n",
    "testing = databc[500:750,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model outside kFold method ( _single training and validation_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer() #Starting clock to count time for training\n",
    "\n",
    "weights = modelBC.fit_model(Training_Set=training, lstFeatures=feats, outCol=10, rate=0.00001, maxIter=1000, reduct=1, ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Model trained in:\",stop-start,\"s\")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelBC.predict(testing, weights)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelBC.evaluate_acc(Preds=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running kFolda Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\luiza\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation # 1  Results:\n",
      "Correct Predictions:  126\n",
      "Total Predictions:  136\n",
      "Accuracy Percentage:  92.6471 %\n",
      "Sensitivity Percentage:  88.5246 %\n",
      "\n",
      "Segmentation # 2  Results:\n",
      "Correct Predictions:  132\n",
      "Total Predictions:  136\n",
      "Accuracy Percentage:  97.0588 %\n",
      "Sensitivity Percentage:  98.4375 %\n",
      "\n",
      "Segmentation # 3  Results:\n",
      "Correct Predictions:  135\n",
      "Total Predictions:  136\n",
      "Accuracy Percentage:  99.2647 %\n",
      "Sensitivity Percentage:  97.9592 %\n",
      "\n",
      "Segmentation # 4  Results:\n",
      "Correct Predictions:  133\n",
      "Total Predictions:  136\n",
      "Accuracy Percentage:  97.7941 %\n",
      "Sensitivity Percentage:  93.3333 %\n",
      "\n",
      "Segmentation # 5  Results:\n",
      "Correct Predictions:  136\n",
      "Total Predictions:  139\n",
      "Accuracy Percentage:  97.8417 %\n",
      "Sensitivity Percentage:  91.4286 %\n",
      "\n",
      "##### RESULTS FOR THE kFOLD CROSS VALIDATION WERE #####\n",
      "Average Accuracy: 96.921 %\n",
      "Average Sensitivity: 93.937 %\n",
      "kFold validation for Breast Cancer data ser performed in: 24.679846299999554 s\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer() #Starting clock\n",
    "\n",
    "modelBC.kfold(k=5,data=databc, LstFeatures=feats, OutCol=10, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"kFold validation for Breast Cancer data ser performed in:\",stop-start,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Wine Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the red wine quality data set, no instances with incorrect of missing values were found. Therefore, no example was removed from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing/anomalous 'entries' in the data set:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.4  ,  0.7  ,  0.   , ...,  0.56 ,  9.4  ,  5.   ],\n",
       "       [ 7.8  ,  0.88 ,  0.   , ...,  0.68 ,  9.8  ,  5.   ],\n",
       "       [ 7.8  ,  0.76 ,  0.04 , ...,  0.65 ,  9.8  ,  5.   ],\n",
       "       ...,\n",
       "       [ 6.3  ,  0.51 ,  0.13 , ...,  0.75 , 11.   ,  6.   ],\n",
       "       [ 5.9  ,  0.645,  0.12 , ...,  0.71 , 10.2  ,  5.   ],\n",
       "       [ 6.   ,  0.31 ,  0.47 , ...,  0.66 , 11.   ,  6.   ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data and remove missing values\n",
    "filename = 'winequality_red.csv'\n",
    "datawine = Read_File_DF(filename, separation=';', head=0, replace=[\"?\"], drop=False)\n",
    "datawine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, the intercept value weights $ w_{0}$ were added as a new column to the beginning of the matrix. The resulting data matrix will have6 1599 rows and 12 columns: 10 features plus 1 intercept and the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  7.4  ,  0.7  , ...,  0.56 ,  9.4  ,  0.   ],\n",
       "       [ 1.   ,  7.8  ,  0.88 , ...,  0.68 ,  9.8  ,  0.   ],\n",
       "       [ 1.   ,  7.8  ,  0.76 , ...,  0.65 ,  9.8  ,  0.   ],\n",
       "       ...,\n",
       "       [ 1.   ,  6.3  ,  0.51 , ...,  0.75 , 11.   ,  1.   ],\n",
       "       [ 1.   ,  5.9  ,  0.645, ...,  0.71 , 10.2  ,  0.   ],\n",
       "       [ 1.   ,  6.   ,  0.31 , ...,  0.66 , 11.   ,  1.   ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datawine = preproc(datawine, Index1=11, Index2=None, Flag=False)\n",
    "datawine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1      2     3     4      5     6      7        8     9     10  \\\n",
      "0     1.0   7.4  0.700  0.00   1.9  0.076  11.0   34.0  0.99780  3.51  0.56   \n",
      "1     1.0   7.8  0.880  0.00   2.6  0.098  25.0   67.0  0.99680  3.20  0.68   \n",
      "2     1.0   7.8  0.760  0.04   2.3  0.092  15.0   54.0  0.99700  3.26  0.65   \n",
      "3     1.0  11.2  0.280  0.56   1.9  0.075  17.0   60.0  0.99800  3.16  0.58   \n",
      "4     1.0   7.4  0.700  0.00   1.9  0.076  11.0   34.0  0.99780  3.51  0.56   \n",
      "5     1.0   7.4  0.660  0.00   1.8  0.075  13.0   40.0  0.99780  3.51  0.56   \n",
      "6     1.0   7.9  0.600  0.06   1.6  0.069  15.0   59.0  0.99640  3.30  0.46   \n",
      "7     1.0   7.3  0.650  0.00   1.2  0.065  15.0   21.0  0.99460  3.39  0.47   \n",
      "8     1.0   7.8  0.580  0.02   2.0  0.073   9.0   18.0  0.99680  3.36  0.57   \n",
      "9     1.0   7.5  0.500  0.36   6.1  0.071  17.0  102.0  0.99780  3.35  0.80   \n",
      "10    1.0   6.7  0.580  0.08   1.8  0.097  15.0   65.0  0.99590  3.28  0.54   \n",
      "11    1.0   7.5  0.500  0.36   6.1  0.071  17.0  102.0  0.99780  3.35  0.80   \n",
      "12    1.0   5.6  0.615  0.00   1.6  0.089  16.0   59.0  0.99430  3.58  0.52   \n",
      "13    1.0   7.8  0.610  0.29   1.6  0.114   9.0   29.0  0.99740  3.26  1.56   \n",
      "14    1.0   8.9  0.620  0.18   3.8  0.176  52.0  145.0  0.99860  3.16  0.88   \n",
      "15    1.0   8.9  0.620  0.19   3.9  0.170  51.0  148.0  0.99860  3.17  0.93   \n",
      "16    1.0   8.5  0.280  0.56   1.8  0.092  35.0  103.0  0.99690  3.30  0.75   \n",
      "17    1.0   8.1  0.560  0.28   1.7  0.368  16.0   56.0  0.99680  3.11  1.28   \n",
      "18    1.0   7.4  0.590  0.08   4.4  0.086   6.0   29.0  0.99740  3.38  0.50   \n",
      "19    1.0   7.9  0.320  0.51   1.8  0.341  17.0   56.0  0.99690  3.04  1.08   \n",
      "20    1.0   8.9  0.220  0.48   1.8  0.077  29.0   60.0  0.99680  3.39  0.53   \n",
      "21    1.0   7.6  0.390  0.31   2.3  0.082  23.0   71.0  0.99820  3.52  0.65   \n",
      "22    1.0   7.9  0.430  0.21   1.6  0.106  10.0   37.0  0.99660  3.17  0.91   \n",
      "23    1.0   8.5  0.490  0.11   2.3  0.084   9.0   67.0  0.99680  3.17  0.53   \n",
      "24    1.0   6.9  0.400  0.14   2.4  0.085  21.0   40.0  0.99680  3.43  0.63   \n",
      "25    1.0   6.3  0.390  0.16   1.4  0.080  11.0   23.0  0.99550  3.34  0.56   \n",
      "26    1.0   7.6  0.410  0.24   1.8  0.080   4.0   11.0  0.99620  3.28  0.59   \n",
      "27    1.0   7.9  0.430  0.21   1.6  0.106  10.0   37.0  0.99660  3.17  0.91   \n",
      "28    1.0   7.1  0.710  0.00   1.9  0.080  14.0   35.0  0.99720  3.47  0.55   \n",
      "29    1.0   7.8  0.645  0.00   2.0  0.082   8.0   16.0  0.99640  3.38  0.59   \n",
      "...   ...   ...    ...   ...   ...    ...   ...    ...      ...   ...   ...   \n",
      "1569  1.0   6.2  0.510  0.14   1.9  0.056  15.0   34.0  0.99396  3.48  0.57   \n",
      "1570  1.0   6.4  0.360  0.53   2.2  0.230  19.0   35.0  0.99340  3.37  0.93   \n",
      "1571  1.0   6.4  0.380  0.14   2.2  0.038  15.0   25.0  0.99514  3.44  0.65   \n",
      "1572  1.0   7.3  0.690  0.32   2.2  0.069  35.0  104.0  0.99632  3.33  0.51   \n",
      "1573  1.0   6.0  0.580  0.20   2.4  0.075  15.0   50.0  0.99467  3.58  0.67   \n",
      "1574  1.0   5.6  0.310  0.78  13.9  0.074  23.0   92.0  0.99677  3.39  0.48   \n",
      "1575  1.0   7.5  0.520  0.40   2.2  0.060  12.0   20.0  0.99474  3.26  0.64   \n",
      "1576  1.0   8.0  0.300  0.63   1.6  0.081  16.0   29.0  0.99588  3.30  0.78   \n",
      "1577  1.0   6.2  0.700  0.15   5.1  0.076  13.0   27.0  0.99622  3.54  0.60   \n",
      "1578  1.0   6.8  0.670  0.15   1.8  0.118  13.0   20.0  0.99540  3.42  0.67   \n",
      "1579  1.0   6.2  0.560  0.09   1.7  0.053  24.0   32.0  0.99402  3.54  0.60   \n",
      "1580  1.0   7.4  0.350  0.33   2.4  0.068   9.0   26.0  0.99470  3.36  0.60   \n",
      "1581  1.0   6.2  0.560  0.09   1.7  0.053  24.0   32.0  0.99402  3.54  0.60   \n",
      "1582  1.0   6.1  0.715  0.10   2.6  0.053  13.0   27.0  0.99362  3.57  0.50   \n",
      "1583  1.0   6.2  0.460  0.29   2.1  0.074  32.0   98.0  0.99578  3.33  0.62   \n",
      "1584  1.0   6.7  0.320  0.44   2.4  0.061  24.0   34.0  0.99484  3.29  0.80   \n",
      "1585  1.0   7.2  0.390  0.44   2.6  0.066  22.0   48.0  0.99494  3.30  0.84   \n",
      "1586  1.0   7.5  0.310  0.41   2.4  0.065  34.0   60.0  0.99492  3.34  0.85   \n",
      "1587  1.0   5.8  0.610  0.11   1.8  0.066  18.0   28.0  0.99483  3.55  0.66   \n",
      "1588  1.0   7.2  0.660  0.33   2.5  0.068  34.0  102.0  0.99414  3.27  0.78   \n",
      "1589  1.0   6.6  0.725  0.20   7.8  0.073  29.0   79.0  0.99770  3.29  0.54   \n",
      "1590  1.0   6.3  0.550  0.15   1.8  0.077  26.0   35.0  0.99314  3.32  0.82   \n",
      "1591  1.0   5.4  0.740  0.09   1.7  0.089  16.0   26.0  0.99402  3.67  0.56   \n",
      "1592  1.0   6.3  0.510  0.13   2.3  0.076  29.0   40.0  0.99574  3.42  0.75   \n",
      "1593  1.0   6.8  0.620  0.08   1.9  0.068  28.0   38.0  0.99651  3.42  0.82   \n",
      "1594  1.0   6.2  0.600  0.08   2.0  0.090  32.0   44.0  0.99490  3.45  0.58   \n",
      "1595  1.0   5.9  0.550  0.10   2.2  0.062  39.0   51.0  0.99512  3.52  0.76   \n",
      "1596  1.0   6.3  0.510  0.13   2.3  0.076  29.0   40.0  0.99574  3.42  0.75   \n",
      "1597  1.0   5.9  0.645  0.12   2.0  0.075  32.0   44.0  0.99547  3.57  0.71   \n",
      "1598  1.0   6.0  0.310  0.47   3.6  0.067  18.0   42.0  0.99549  3.39  0.66   \n",
      "\n",
      "        11   12  \n",
      "0      9.4  0.0  \n",
      "1      9.8  0.0  \n",
      "2      9.8  0.0  \n",
      "3      9.8  1.0  \n",
      "4      9.4  0.0  \n",
      "5      9.4  0.0  \n",
      "6      9.4  0.0  \n",
      "7     10.0  1.0  \n",
      "8      9.5  1.0  \n",
      "9     10.5  0.0  \n",
      "10     9.2  0.0  \n",
      "11    10.5  0.0  \n",
      "12     9.9  0.0  \n",
      "13     9.1  0.0  \n",
      "14     9.2  0.0  \n",
      "15     9.2  0.0  \n",
      "16    10.5  1.0  \n",
      "17     9.3  0.0  \n",
      "18     9.0  0.0  \n",
      "19     9.2  1.0  \n",
      "20     9.4  1.0  \n",
      "21     9.7  0.0  \n",
      "22     9.5  0.0  \n",
      "23     9.4  0.0  \n",
      "24     9.7  1.0  \n",
      "25     9.3  0.0  \n",
      "26     9.5  0.0  \n",
      "27     9.5  0.0  \n",
      "28     9.4  0.0  \n",
      "29     9.8  1.0  \n",
      "...    ...  ...  \n",
      "1569  11.5  1.0  \n",
      "1570  12.4  1.0  \n",
      "1571  11.1  1.0  \n",
      "1572   9.5  0.0  \n",
      "1573  12.5  1.0  \n",
      "1574  10.5  1.0  \n",
      "1575  11.8  1.0  \n",
      "1576  10.8  1.0  \n",
      "1577  11.9  1.0  \n",
      "1578  11.3  1.0  \n",
      "1579  11.3  0.0  \n",
      "1580  11.9  1.0  \n",
      "1581  11.3  0.0  \n",
      "1582  11.9  0.0  \n",
      "1583   9.8  0.0  \n",
      "1584  11.6  1.0  \n",
      "1585  11.5  1.0  \n",
      "1586  11.4  1.0  \n",
      "1587  10.9  1.0  \n",
      "1588  12.8  1.0  \n",
      "1589   9.2  0.0  \n",
      "1590  11.6  1.0  \n",
      "1591  11.6  1.0  \n",
      "1592  11.0  1.0  \n",
      "1593   9.5  1.0  \n",
      "1594  10.5  0.0  \n",
      "1595  11.2  1.0  \n",
      "1596  11.0  1.0  \n",
      "1597  10.2  0.0  \n",
      "1598  11.0  1.0  \n",
      "\n",
      "[1599 rows x 13 columns]\n",
      "           0            1            2            3            4   \\\n",
      "count  1599.0  1599.000000  1599.000000  1599.000000  1599.000000   \n",
      "mean      1.0     8.319637     0.527821     0.270976     2.538806   \n",
      "std       0.0     1.741096     0.179060     0.194801     1.409928   \n",
      "min       1.0     4.600000     0.120000     0.000000     0.900000   \n",
      "0%        1.0     4.600000     0.120000     0.000000     0.900000   \n",
      "25%       1.0     7.100000     0.390000     0.090000     1.900000   \n",
      "50%       1.0     7.900000     0.520000     0.260000     2.200000   \n",
      "75%       1.0     9.200000     0.640000     0.420000     2.600000   \n",
      "max       1.0    15.900000     1.580000     1.000000    15.500000   \n",
      "\n",
      "                5            6            7            8            9   \\\n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  1599.000000   \n",
      "mean      0.087467    15.874922    46.467792     0.996747     3.311113   \n",
      "std       0.047065    10.460157    32.895324     0.001887     0.154386   \n",
      "min       0.012000     1.000000     6.000000     0.990070     2.740000   \n",
      "0%        0.012000     1.000000     6.000000     0.990070     2.740000   \n",
      "25%       0.070000     7.000000    22.000000     0.995600     3.210000   \n",
      "50%       0.079000    14.000000    38.000000     0.996750     3.310000   \n",
      "75%       0.090000    21.000000    62.000000     0.997835     3.400000   \n",
      "max       0.611000    72.000000   289.000000     1.003690     4.010000   \n",
      "\n",
      "                10           11           12  \n",
      "count  1599.000000  1599.000000  1599.000000  \n",
      "mean      0.658149    10.422983     0.534709  \n",
      "std       0.169507     1.065668     0.498950  \n",
      "min       0.330000     8.400000     0.000000  \n",
      "0%        0.330000     8.400000     0.000000  \n",
      "25%       0.550000     9.500000     0.000000  \n",
      "50%       0.620000    10.200000     1.000000  \n",
      "75%       0.730000    11.100000     1.000000  \n",
      "max       2.000000    14.900000     1.000000  \n",
      "    0         1         2         3         4         5         6         7   \\\n",
      "0  NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "1  NaN  1.000000 -0.278282  0.661708  0.220701  0.250904 -0.175137 -0.088417   \n",
      "2  NaN -0.278282  1.000000 -0.610259  0.032386  0.158770  0.021163  0.094110   \n",
      "3  NaN  0.661708 -0.610259  1.000000  0.176417  0.112577 -0.076452  0.009400   \n",
      "4  NaN  0.220701  0.032386  0.176417  1.000000  0.212959  0.074618  0.145375   \n",
      "5  NaN  0.250904  0.158770  0.112577  0.212959  1.000000  0.000805  0.130033   \n",
      "6  NaN -0.175137  0.021163 -0.076452  0.074618  0.000805  1.000000  0.789698   \n",
      "7  NaN -0.088417  0.094110  0.009400  0.145375  0.130033  0.789698  1.000000   \n",
      "8  NaN  0.623071  0.025014  0.352285  0.422266  0.411390 -0.041178  0.129332   \n",
      "9  NaN -0.706674  0.233572 -0.548026 -0.089971 -0.234361  0.115679 -0.009841   \n",
      "10 NaN  0.212654 -0.325584  0.331074  0.038332  0.020825  0.045862 -0.000504   \n",
      "11 NaN -0.066576 -0.224932  0.096456  0.116548 -0.284504 -0.081367 -0.257806   \n",
      "12 NaN  0.081062 -0.328315  0.158191  0.013854 -0.173797 -0.053447 -0.198455   \n",
      "\n",
      "          8         9         10        11        12  \n",
      "0        NaN       NaN       NaN       NaN       NaN  \n",
      "1   0.623071 -0.706674  0.212654 -0.066576  0.081062  \n",
      "2   0.025014  0.233572 -0.325584 -0.224932 -0.328315  \n",
      "3   0.352285 -0.548026  0.331074  0.096456  0.158191  \n",
      "4   0.422266 -0.089971  0.038332  0.116548  0.013854  \n",
      "5   0.411390 -0.234361  0.020825 -0.284504 -0.173797  \n",
      "6  -0.041178  0.115679  0.045862 -0.081367 -0.053447  \n",
      "7   0.129332 -0.009841 -0.000504 -0.257806 -0.198455  \n",
      "8   1.000000 -0.312055  0.161478 -0.462445 -0.164363  \n",
      "9  -0.312055  1.000000 -0.080306  0.179932 -0.005169  \n",
      "10  0.161478 -0.080306  1.000000  0.207330  0.333970  \n",
      "11 -0.462445  0.179932  0.207330  1.000000  0.443816  \n",
      "12 -0.164363 -0.005169  0.333970  0.443816  1.000000  \n"
     ]
    }
   ],
   "source": [
    "Data_Stats(datawine, QQ_DD = True, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Calling constructor for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelWine = logisticReg(datawine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of features to be selected (_always between 0-10 with the 0 included_ - $w_{0}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [0,1,2,3,4,5,6,7,8,9,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the set/subset of examples to be used in training and validating the model ( _for uses outside kFold method_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = datawine[250:750,:]\n",
    "testing = datawine[1000:1350,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model outside kFold method ( _single training and validation_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer() #Starting clock to count time for training\n",
    "\n",
    "weights = modelWine.fit_model(Training_Set=training, lstFeatures=feats, outCol=12, rate=0.00001, maxIter=100, reduct=1, ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Model trained in:\",stop-start,\"s\")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelWine.predict(testing, weights)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelWine.evaluate_acc(Preds=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running kFolda Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\luiza\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation # 1  Results:\n",
      "Correct Predictions:  197\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  61.7555 %\n",
      "Sensitivity Percentage:  4.8387 %\n",
      "\n",
      "Segmentation # 2  Results:\n",
      "Correct Predictions:  207\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  64.8903 %\n",
      "Sensitivity Percentage:  93.8462 %\n",
      "\n",
      "Segmentation # 3  Results:\n",
      "Correct Predictions:  167\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  52.3511 %\n",
      "Sensitivity Percentage:  98.0132 %\n",
      "\n",
      "Segmentation # 4  Results:\n",
      "Correct Predictions:  225\n",
      "Total Predictions:  319\n",
      "Accuracy Percentage:  70.5329 %\n",
      "Sensitivity Percentage:  93.2432 %\n",
      "\n",
      "Segmentation # 5  Results:\n",
      "Correct Predictions:  163\n",
      "Total Predictions:  323\n",
      "Accuracy Percentage:  50.4644 %\n",
      "Sensitivity Percentage:  95.092 %\n",
      "\n",
      "##### RESULTS FOR THE kFOLD CROSS VALIDATION WERE #####\n",
      "Average Accuracy: 59.999 %\n",
      "Average Sensitivity: 77.007 %\n",
      "kFold validation for Wine Data set performed in: 475.23168780000015 s\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer() #Starting clock\n",
    "\n",
    "modelWine.kfold(k=5,data=datawine, LstFeatures=feats, OutCol=12, Rate=0.00001, MaxIter=1000, Reduct=1, Ran=False)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"kFold validation for Wine Data set performed in:\",stop-start,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
